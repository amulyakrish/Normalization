{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftXRSUxX18Lz",
        "outputId": "b7c1d214-a1d9-4bc3-8a04-ab7a54def487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Functional Dependencies (or 'exit'): Teacher,Subject -> Student\n",
            "Input Functional Dependencies (or 'exit'): exit\n",
            "Key (can be composite): Teacher,Subject\n",
            "Choice of the highest normal form to reach (1: 1NF, 2: 2NF, 3: 3NF, B: BCNF, 4: 4NF, 5: 5NF): B\n",
            "in 1NF\n",
            "checking for 2NF...\n",
            "Table is in 2NF\n",
            "table Teacher_Subject is in 3NF\n",
            "============== Table is in BCNF ===========\n"
          ]
        }
      ],
      "source": [
        "from typing import DefaultDict\n",
        "from numpy import left_shift\n",
        "import csv\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "global_map={}\n",
        "def read_csv_file(file_name):\n",
        "    with open(file_name, newline='', encoding='utf-8-sig') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        data = [dict((k.strip(), v) for k, v in row.items()) for row in reader]\n",
        "    return data\n",
        "\n",
        "def parse_functional_dependencies():\n",
        "    fds = []\n",
        "    while True:\n",
        "        fd_input = input(\"Input Functional Dependencies (or 'exit'): \")\n",
        "        if fd_input == \"exit\":\n",
        "            break\n",
        "        fd = fd_input.split('->')\n",
        "        fds.append((set(x.strip() for x in fd[0].split(',')), set(x.strip() for x in fd[1].split(','))))\n",
        "    return fds\n",
        "\n",
        "def decompose_to_1NF(fds,data,multi_value_attributes):\n",
        "\n",
        "\n",
        "\n",
        "  original_df = pd.DataFrame(data)\n",
        "  #print(original_df)\n",
        "\n",
        "  for attr in multi_value_attributes:\n",
        "    # Split the \"Courses\" column into a list of courses\n",
        "    original_df[attr] = original_df[attr].str.split(',')\n",
        "\n",
        "    # Explode the \"Courses\" column to create multiple rows for each course\n",
        "    result_df = original_df.explode(attr).reset_index(drop=True)\n",
        "\n",
        "    original_df = result_df\n",
        "\n",
        "\n",
        "  #print(\"Resulting DataFrame:\")\n",
        "  #print(result_df)\n",
        "  data = result_df.to_dict(orient='records')\n",
        "  return data\n",
        "  #print(data)\n",
        "\n",
        "\n",
        "\n",
        "def find_normal_form(data, fds, key,target_nf_choice, mvds):\n",
        "    #to-do\n",
        "    #target_nf = 3\n",
        "    key = set(key)\n",
        "    all_attributes = set(data[0].keys())\n",
        "\n",
        "    table_fds={}\n",
        "    tables_of_2nf_dfs={}\n",
        "    # Check for 1NF\n",
        "    multi_value_attributes = is_in_1NF(data)\n",
        "    if multi_value_attributes == set():\n",
        "      print(\"in 1NF\")\n",
        "      print(\"checking for 2NF...\")\n",
        "    else:\n",
        "      print(\"Not in 1NF\")\n",
        "      #print(\"multi_value_attributes : {}\",multi_value_attributes)\n",
        "      data = decompose_to_1NF(fds,data,multi_value_attributes)\n",
        "      print(\"printing table after decomposition...\")\n",
        "      print(data)\n",
        "\n",
        "    if target_nf_choice >=  '2':\n",
        "      # Check for 2NF\n",
        "      violated_fds,correct_fds = is_in_2NF(fds,key)\n",
        "      #print(\"=========violated==========\")\n",
        "      #print(violated_fds)\n",
        "      #print(\"=========correct_fds==========\")\n",
        "      #print(correct_fds)\n",
        "      if violated_fds == []:\n",
        "        print(\"Table is in 2NF\")\n",
        "        table_fds = {}\n",
        "        tables_of_2nf_dfs = {}\n",
        "        #print(\"============== fds ==========\")\n",
        "        #print(fds)\n",
        "        #print(\"============== key ==========\")\n",
        "        #print(key)\n",
        "        table_fds[\"_\".join(key)] = fds\n",
        "        tables_of_2nf_dfs[\"_\".join(key)] = converDataToDataFrame(data)\n",
        "        populateGlobalMap(fds,key,data,violated_fds,correct_fds)\n",
        "        if target_nf_choice >=  '3':\n",
        "          table_fds,tables_of_2nf_dfs = is_in_3NF(table_fds,tables_of_2nf_dfs,key)\n",
        "      else:\n",
        "        print(\"Table is not in 2NF\")\n",
        "        table_fds,tables_of_2nf_dfs = decompose_to_2NF(data,key,violated_fds,correct_fds,fds)\n",
        "        for k,v in tables_of_2nf_dfs.items():\n",
        "          print(\"Primary Key : \",k)\n",
        "          print(v)\n",
        "        if target_nf_choice >=  '3':\n",
        "          table_fds,tables_of_2nf_dfs = is_in_3NF(table_fds,tables_of_2nf_dfs,key)\n",
        "          for k,v in tables_of_2nf_dfs.items():\n",
        "            print(\"Primary Key : \",k)\n",
        "            print(v)\n",
        "    if(target_nf_choice ==  'B'):\n",
        "      #print(\"======= once more =======\")\n",
        "      #print(table_fds)\n",
        "      #print(\"======= once more =======\")\n",
        "      #print(tables_of_2nf_dfs)\n",
        "      #print(\"choice is B\")\n",
        "      problematic_fds = is_in_BCNF(table_fds, tables_of_2nf_dfs,key)\n",
        "      if problematic_fds == {}:\n",
        "        print(\"============== Table is in BCNF ===========\")\n",
        "        #print(tables_of_2nf_dfs.values())\n",
        "      else:\n",
        "        #print(\"==== problematic_fds =======\")\n",
        "        #print(problematic_fds)\n",
        "        print(\"============== Table is not in BCNF ===========\")\n",
        "        table_fds, tables_of_2nf_dfs = decompose_to_BCNF(problematic_fds,table_fds, tables_of_2nf_dfs,key)\n",
        "        for k,v in tables_of_2nf_dfs.items():\n",
        "          print(\"Primary Key : \",k)\n",
        "          print(v)\n",
        "        #print(\"================after resolving ======\")\n",
        "        #print(table_fds)\n",
        "        #print(tables_of_2nf_dfs)\n",
        "    if(target_nf_choice ==  '4'):\n",
        "      is_in_4NF(table_fds,tables_of_2nf_dfs,mvds,key)\n",
        "\n",
        "\n",
        "def is_in_4NF(table_fds,tables_of_2nf_dfs,mvds,key):\n",
        "      #converting mvd into map of  {k|v1,v2}\n",
        "      mvd_map = {}\n",
        "      for each_mvd in mvds:\n",
        "        if next(iter(each_mvd[0])) not in mvd_map:\n",
        "          mvd_map[next(iter(each_mvd[0]))] = []\n",
        "        mvd_map[next(iter(each_mvd[0]))].append(next(iter(each_mvd[1])))\n",
        "\n",
        "      #print(\"mvd_map : \",mvd_map)\n",
        "      flag = 0\n",
        "      for table_name,df in tables_of_2nf_dfs.items():\n",
        "        table_attrs = df.keys()\n",
        "        table_mvds={}\n",
        "        for mvd in mvds:\n",
        "          mvd_attrs = mvd[1].union(mvd[0])\n",
        "          if mvd_attrs.issubset(table_attrs):\n",
        "            table_mvds.setdefault(table_name,[]).append(mvd)\n",
        "        #print(table_mvds)\n",
        "        B,C = mvd_map[table_name]\n",
        "        if B not in key and C not in key:\n",
        "          print(\"Table is not in 4NF\")\n",
        "          flag=1\n",
        "          orig_table = tables_of_2nf_dfs[table_name]\n",
        "          print(\"========Tables=========\")\n",
        "          cols = []\n",
        "          cols.append(table_name)\n",
        "          cols.append(B)\n",
        "          print(orig_table[cols])\n",
        "          cols = []\n",
        "          cols.append(table_name)\n",
        "          cols.append(C)\n",
        "          print(orig_table[cols])\n",
        "\n",
        "      if flag == 0:\n",
        "        print(\"Table is in 4NF\")\n",
        "\n",
        "        #mvd_map :  {'Ename': ['Pname', 'Dname']}\n",
        "        #{'Ename': [({'Ename'}, {'Pname'}), ({'Ename'}, {'Dname'})]}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def decompose_4nf(data, fds, mvds):\n",
        "    # Implementing basic 4NF decomposition using given MVDs.\n",
        "    all_attrs = set(data[0].keys())\n",
        "    decompositions = []\n",
        "    for mvd in mvds:\n",
        "        left, right = mvd\n",
        "        decompositions.append(left.union(right))\n",
        "        all_attrs -= right\n",
        "\n",
        "    decompositions.append(all_attrs)\n",
        "    return decompositions\n",
        "\n",
        "def decompose_to_5nf(data, fds):\n",
        "    # Identify join dependencies and relevant attributes\n",
        "    join_dependencies, relevant_attributes = identify_join_dependencies(fds)\n",
        "\n",
        "    # Perform lossless-join decomposition based on identified JDs\n",
        "    decomposed_tables = lossless_join_decomposition(data, join_dependencies, relevant_attributes)\n",
        "\n",
        "    return decomposed_tables\n",
        "\n",
        "def identify_join_dependencies(fds):\n",
        "    join_dependencies = []  # List of identified join dependencies\n",
        "    relevant_attributes = set()  # Set of relevant attributes\n",
        "\n",
        "    # Create a dictionary to store the attributes that are functionally dependent on each key\n",
        "    fd_dict = {}\n",
        "\n",
        "    for fd in fds:\n",
        "        left, right = fd\n",
        "        # Check if the right-hand side is a subset of the key (i.e., it determines all key attributes)\n",
        "        if left.issubset(right):\n",
        "            join_dependencies.append(fd)\n",
        "            relevant_attributes.update(right)\n",
        "        else:\n",
        "            # Update the dictionary with attributes that are functionally dependent on the left-hand side\n",
        "            for attr in left:\n",
        "                if attr in fd_dict:\n",
        "                    fd_dict[attr].update(right)\n",
        "                else:\n",
        "                    fd_dict[attr] = set(right)\n",
        "\n",
        "    # Find additional join dependencies using the dictionary\n",
        "    for attr, dependent_attrs in fd_dict.items():\n",
        "        if attr in relevant_attributes:\n",
        "            for dep_attr in dependent_attrs:\n",
        "                if dep_attr != attr:\n",
        "                    # If dep_attr is not already in join_dependencies, add it as a join dependency\n",
        "                    if not any(dep_fd == (set([attr]), set([dep_attr])) for dep_fd in join_dependencies):\n",
        "                        join_dependencies.append((set([attr]), set([dep_attr])))\n",
        "\n",
        "    return join_dependencies, relevant_attributes\n",
        "\n",
        "\n",
        "def lossless_join_decomposition(data, join_dependencies, relevant_attributes):\n",
        "    decomposed_tables = []\n",
        "\n",
        "    # Create a set of attributes that have not been covered by any decomposition yet\n",
        "    uncovered_attributes = set(data[0].keys())\n",
        "\n",
        "    while uncovered_attributes:\n",
        "        # Choose a set of attributes to create a new decomposed table\n",
        "        chosen_attributes = choose_attributes(uncovered_attributes, relevant_attributes)\n",
        "        decomposed_tables.append(chosen_attributes)\n",
        "        uncovered_attributes -= chosen_attributes\n",
        "\n",
        "    return decomposed_tables\n",
        "\n",
        "def choose_attributes(uncovered_attributes, relevant_attributes):\n",
        "    chosen_attributes = set()\n",
        "\n",
        "    # Iterate through the relevant attributes to find a suitable set of attributes\n",
        "    for attr in relevant_attributes:\n",
        "        if attr in uncovered_attributes:\n",
        "            chosen_attributes.add(attr)\n",
        "            break\n",
        "\n",
        "    # If no relevant attribute is left to choose, pick any uncovered attribute\n",
        "    if not chosen_attributes:\n",
        "        chosen_attributes.add(uncovered_attributes.pop())\n",
        "\n",
        "    return chosen_attributes\n",
        "\n",
        "\n",
        "# Example of usage:\n",
        "# Define your functional dependencies (FDs)\n",
        "#fds = [({'A'}, {'B'}), ({'B', 'C'}, {'D', 'E'}), ({'D'}, {'F'}), ...]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def decompose_to_BCNF(problematic_fds,table_fds, tables_of_2nf_dfs,key):\n",
        "  print(\"================  decompose_to_BCNF ============\")\n",
        "  #print(problematic_fds)\n",
        "  #print(table_fds)\n",
        "  #print(tables_of_2nf_dfs)\n",
        "  #print(key)\n",
        "\n",
        "  for problematic_table_name,problematic_fd_indices in problematic_fds.items():\n",
        "    orig_df = tables_of_2nf_dfs[problematic_table_name]\n",
        "    for ind in problematic_fd_indices:\n",
        "      each_problematic_fd = table_fds[problematic_table_name][ind]\n",
        "\n",
        "\n",
        "      X = each_problematic_fd[0]\n",
        "      Y = each_problematic_fd[1]\n",
        "\n",
        "      #resolve\n",
        "      #include new table\n",
        "      l_key = '_'.join(X)\n",
        "      if l_key not in table_fds:\n",
        "        table_fds[l_key] = Y.union(X)\n",
        "\n",
        "      #include new tables' df\n",
        "      if l_key not in tables_of_2nf_dfs:\n",
        "        tables_of_2nf_dfs[l_key] = orig_df[list(table_fds[l_key])]\n",
        "\n",
        "    #remove p_fd from original table\n",
        "    problematic_fd_indices.sort(reverse=True)\n",
        "    for ind in problematic_fd_indices:\n",
        "      table_fds[problematic_table_name].pop(ind)\n",
        "\n",
        "  return table_fds, tables_of_2nf_dfs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def is_in_BCNF(table_fds,tables_of_2nf_dfs, candidate_key):\n",
        "  problematic_fds = {}\n",
        "  for table_name, fds in table_fds.items():\n",
        "    # Initialize an empty set for candidate key\n",
        "    for i, fd in enumerate(fds):\n",
        "      left_hand_side = fd[0]\n",
        "      # Check if the left-hand side of the FD is not a superkey\n",
        "      if not is_superkey(candidate_key, fds[:i]):\n",
        "        problematic_fds.setdefault(table_name, []).append(i)\n",
        "      candidate_key.update(left_hand_side)\n",
        "  return problematic_fds\n",
        "\n",
        "def is_superkey(candidate_key, fds):\n",
        "    # Check if candidate_key is a superkey by ensuring it determines all attributes in the table\n",
        "    for fd in fds:\n",
        "        if not candidate_key.issuperset(fd[1]):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def populateGlobalMap(fds,primary_key_set,data,violated_fds,correct_fds):\n",
        "  global global_map\n",
        "  # Include the main table with the primary key and remaining attributes\n",
        "  map = {}\n",
        "  for fd in fds:\n",
        "  #fd[0] -> left\n",
        "  #fd[1] -> right\n",
        "   if fd[0].issubset(primary_key_set) and not fd[0] == primary_key_set:\n",
        "     map[\"_\".join(list(fd[0]))]=None\n",
        "     #and\n",
        "   elif fd[0] == primary_key_set:\n",
        "     map[\"_\".join(list(fd[0]))]=None\n",
        "     #tables_of_2nf[\"_\".join(list(fd[0]))]=set()\n",
        "\n",
        "\n",
        "  for vfd in violated_fds:\n",
        "      for every_val_in_rhs in vfd[1]:\n",
        "        if every_val_in_rhs not in map:\n",
        "          map[every_val_in_rhs]=\"_\".join(vfd[0])\n",
        "\n",
        "  for cfd in correct_fds:\n",
        "    for every_val_in_rhs in cfd[1]:\n",
        "      map[every_val_in_rhs]=\"_\".join(cfd[0])\n",
        "\n",
        "  for vfd in violated_fds:\n",
        "      if '_'.join(vfd[0]) not in map:\n",
        "        map['_'.join(vfd[0])]=None\n",
        "      non_primes_in_lhs = []\n",
        "      for attr in vfd[0]:\n",
        "        if attr not in primary_key_set:\n",
        "          non_primes_in_lhs.append(attr)\n",
        "      for ever_nonprime_attr in non_primes_in_lhs:\n",
        "        map[ever_nonprime_attr] = \"_\".join(vfd[0])\n",
        "  #print(\"========== map ===========\")\n",
        "  #print(map)\n",
        "\n",
        "  global_map = map\n",
        "\n",
        "def converDataToDataFrame(data):\n",
        "  original_df = pd.DataFrame(data)\n",
        "  return original_df\n",
        "def is_in_3NF(table_fds,tables_of_2nf_dfs,key):\n",
        "  #print(\"============ tables_of_2nf_dfs ========\")\n",
        "  #print(tables_of_2nf_dfs)\n",
        "  #{'StudentID': [({'StudentID'}, {'LastName', 'FirstName'})], 'Course': [({'Course'}, {'CourseEnd', 'Professor', 'CourseStart'}), ({'Professor'}, {'ProfessorEmail'})]}\n",
        "  faulty_dependency_indexes = {}\n",
        "  temp_table_fds = deepcopy(table_fds)\n",
        "  for table_name,list_of_fds in table_fds.items():\n",
        "    if list_of_fds == []:\n",
        "      continue\n",
        "    attr_list = set()\n",
        "    for fd in list_of_fds:\n",
        "      attr_list.update(fd[1].union(fd[0]))\n",
        "\n",
        "    transitive_dependencies,faulty_dependency_indexes = is_in_3NF_internal(list_of_fds,attr_list ,table_name, faulty_dependency_indexes,key)\n",
        "    #print(\"======== transitive_dependencies=========\")\n",
        "    #print(transitive_dependencies)\n",
        "    if transitive_dependencies == list():\n",
        "      print(f\"table {table_name} is in 3NF\")\n",
        "    else:\n",
        "      print(f\"table {table_name} is not in 3NF\")\n",
        "    #print(\"======== faulty_dependency_indexes=========\")\n",
        "    #print(faulty_dependency_indexes)\n",
        "    for key,val in faulty_dependency_indexes.items():\n",
        "      faulty_dependency_indexes[key]=list(set(val))\n",
        "    #print(\"========== working till this point ================\")\n",
        "    decompose_to_3NF(transitive_dependencies,temp_table_fds, tables_of_2nf_dfs,faulty_dependency_indexes)\n",
        "  table_fds = temp_table_fds\n",
        "  #print(\"======== final table_fds =========\")\n",
        "  #print(table_fds)\n",
        "  #print(\"======== final tables_of_2nf_dfs hii =========\")\n",
        "  #print(tables_of_2nf_dfs)\n",
        "  return table_fds,tables_of_2nf_dfs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def decompose_to_3NF(transitive_dependencies,temp_table_fds, tables_of_2nf_dfs,faulty_dependency_indexes):\n",
        "\n",
        "\n",
        "  for faulty_table,faulty_fd_indexes in faulty_dependency_indexes.items():\n",
        "    data_df = tables_of_2nf_dfs[faulty_table]\n",
        "    for each_faulty_dependency_index in faulty_fd_indexes:\n",
        "      faulty_fd = temp_table_fds[faulty_table][each_faulty_dependency_index]\n",
        "      #print(\"faulty_fd : \",faulty_fd)\n",
        "\n",
        "      #take right side attributes of faulty_fd\n",
        "      removable_cols = list(faulty_fd[1])\n",
        "\n",
        "      #create new tble faulty_fd[1].unionfaulty_fd[0] with faulty_fd[0] as another pk\n",
        "      table_attrs = faulty_fd[1].union(faulty_fd[0])\n",
        "\n",
        "      #print(\"table_attrs : \",table_attrs)\n",
        "      #print(data_df)\n",
        "      new_df = data_df[list(table_attrs)]\n",
        "      #print(new_df)\n",
        "\n",
        "      tables_of_2nf_dfs[\"_\".join(faulty_fd[0])] = new_df\n",
        "\n",
        "      #update temp_table_fds with new pk key and new fd list\n",
        "      temp_table_fds.setdefault(\"_\".join(faulty_fd[0]), []).append(faulty_fd)\n",
        "\n",
        "\n",
        "      #trim from data\n",
        "      data_df = data_df.drop(columns=removable_cols)\n",
        "      #print(data_df)\n",
        "\n",
        "      #update tables_of_2nf_dfs with new pk key and new df table\n",
        "      tables_of_2nf_dfs[faulty_table] = data_df\n",
        "\n",
        "\n",
        "      #print(tables_of_2nf_dfs)\n",
        "\n",
        "      #print(\" ==== before popping =======\")\n",
        "      #print(temp_table_fds[faulty_table])\n",
        "      temp_table_fds[faulty_table].pop(each_faulty_dependency_index)\n",
        "      #print(temp_table_fds[faulty_table])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #print(\"====================================\")\n",
        "    #print(temp_table_fds)\n",
        "    #print(\"====================================\")\n",
        "    #print(tables_of_2nf_dfs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def decompose_to_2NF(data, primary_key,violated_fds,correct_fds,fds):\n",
        "    global global_map\n",
        "    decomposed_relations = []\n",
        "    tables_of_2nf = {}\n",
        "    table_fds = {}\n",
        "\n",
        "    primary_key_set = set(primary_key)\n",
        "    tables_of_2nf[\"_\".join(primary_key_set)]=primary_key_set\n",
        "    table_fds[\"_\".join(primary_key_set)] = []\n",
        "    #========= violated_fds =========\n",
        "    #[({'Ssn'}, {'Ename'}), ({'Pnumber'}, {'Pname'}), ({'Pnumber'}, {'Plocation'})]\n",
        "    for fd in violated_fds:\n",
        "      # Create a new table\n",
        "      table_attrs = fd[1].union(fd[0])\n",
        "      temp_key = \"_\".join(fd[0])\n",
        "      if temp_key not in tables_of_2nf:\n",
        "        tables_of_2nf[temp_key] = set()\n",
        "      tables_of_2nf[temp_key].update(table_attrs)\n",
        "\n",
        "      if temp_key not in table_fds:\n",
        "        table_fds[temp_key] = list()\n",
        "      table_fds[temp_key].append(fd)\n",
        "\n",
        "\n",
        "\n",
        "    #print(\"******* tables_of_2nf ******\")\n",
        "    #print(tables_of_2nf)\n",
        "\n",
        "\n",
        "    # Include the main table with the primary key and remaining attributes\n",
        "    map = {}\n",
        "    for fd in fds:\n",
        "      #fd[0] -> left\n",
        "      #fd[1] -> right\n",
        "      if fd[0].issubset(primary_key_set) and not fd[0] == primary_key_set:\n",
        "        map[\"_\".join(list(fd[0]))]=None\n",
        "        #and\n",
        "      elif fd[0] == primary_key_set:\n",
        "        map[\"_\".join(list(fd[0]))]=None\n",
        "        #tables_of_2nf[\"_\".join(list(fd[0]))]=set()\n",
        "\n",
        "    for vfd in violated_fds:\n",
        "      for every_val_in_rhs in vfd[1]:\n",
        "        if every_val_in_rhs not in map:\n",
        "          map[every_val_in_rhs]=\"_\".join(vfd[0])\n",
        "\n",
        "    for cfd in correct_fds:\n",
        "      for every_val_in_rhs in cfd[1]:\n",
        "        map[every_val_in_rhs]=\"_\".join(cfd[0])\n",
        "\n",
        "\n",
        "\n",
        "    for vfd in violated_fds:\n",
        "      if '_'.join(vfd[0]) not in map:\n",
        "        map['_'.join(vfd[0])]=None\n",
        "      non_primes_in_lhs = []\n",
        "      for attr in vfd[0]:\n",
        "        if attr not in primary_key_set:\n",
        "          non_primes_in_lhs.append(attr)\n",
        "      for ever_nonprime_attr in non_primes_in_lhs:\n",
        "        map[ever_nonprime_attr] = \"_\".join(vfd[0])\n",
        "\n",
        "    #print(\"========== map ===========\")\n",
        "    #print(map)\n",
        "    #print(data[0].keys())\n",
        "\n",
        "    global_map = map\n",
        "\n",
        "    #print(tables_of_2nf)\n",
        "    for correct_fd in correct_fds:\n",
        "      #[({'Professor'}, {'ProfessorEmail'})]\n",
        "      rhs = correct_fd[1]\n",
        "      #get the table of lhs\n",
        "      set_iterator = iter(rhs)\n",
        "      # Get the first element from the set\n",
        "      first_element = next(set_iterator)\n",
        "      #print(correct_fds)\n",
        "      #print(\"========== first_element ===========\")\n",
        "      #print(first_element)\n",
        "\n",
        "      while map[first_element] is not None:\n",
        "        first_element = map[first_element]\n",
        "\n",
        "      temp_key = first_element\n",
        "      #print(\"========== temp_key ===========\")\n",
        "      #print(temp_key)\n",
        "\n",
        "      #tables_of_2nf[temp_key] =\n",
        "      for item in rhs:\n",
        "        tables_of_2nf[temp_key].add(item)\n",
        "      table_fds[temp_key].append(correct_fd)\n",
        "\n",
        "      #print(\"========== tables_of_2nf ===========\")\n",
        "      #print(tables_of_2nf)\n",
        "\n",
        "\n",
        "    tables_of_2nf_dfs={}\n",
        "\n",
        "    original_df = pd.DataFrame(data)\n",
        "\n",
        "    for table_name,corresponding_cols in tables_of_2nf.items():\n",
        "      tables_of_2nf_dfs[table_name] = original_df[corresponding_cols]\n",
        "\n",
        "    ##print(\"\\n\\n\\n\")\n",
        "    #for table_name,df in tables_of_2nf_dfs.items():\n",
        "\n",
        "      #print(table_name)\n",
        "      #print(\"\\n\")\n",
        "      #print(df)\n",
        "\n",
        "    #print(\"===== table_fds ====\")\n",
        "    #print(table_fds)\n",
        "\n",
        "\n",
        "    ###sending table_fds,tables_of_2nf_dfs for 3NF verification\n",
        "    return table_fds,tables_of_2nf_dfs\n",
        "\n",
        "\n",
        "\n",
        "    #remaining_attrs = set(data[0].keys()).difference(set().union(*decomposed_relations))\n",
        "    #decomposed_relations.append(primary_key_set.union(remaining_attrs))\n",
        "\n",
        "    #return decomposed_relations, remaining_fds\n",
        "\n",
        "def is_in_1NF(data):\n",
        "    # Using a common delimiter ';'\n",
        "    # Modify based on what you think might be used to store multiple values\n",
        "    delimiter = ','\n",
        "    multi_value_attributes = set()\n",
        "    for row in data:\n",
        "        for attribute, value in row.items():\n",
        "            if delimiter in value:\n",
        "                print(f\"Attribute {attribute} contains non-atomic values.\")\n",
        "                multi_value_attributes.add(attribute)\n",
        "    return multi_value_attributes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def is_in_2NF(fds,key):\n",
        "  \"[({'StudentID'}, {'FirstName', 'LastName'})]\"\n",
        "  #print(\"fds : \",fds)\n",
        "  #print(\"key : \",key)\n",
        "  violated_fds = []\n",
        "  correct_fds=[]\n",
        "  for fd in fds:\n",
        "    #print(\"fd1 : \",fd[1])\n",
        "    condition1 = len(fd[0]) < len(key) and fd[0].issubset(key)\n",
        "    if(condition1):\n",
        "      violated_fds.append(fd)\n",
        "      continue\n",
        "    count = len(key)\n",
        "    actualCount = 0\n",
        "    for item in fd[0]:\n",
        "      if item in key:\n",
        "        actualCount += 1\n",
        "    condition2 =  count !=   actualCount and actualCount>=1\n",
        "    if(condition2):\n",
        "      correct_fds.append(fd)\n",
        "      continue\n",
        "\n",
        "    #checking if lhs completely conisst of np attributes, declaring it as a safe fd\n",
        "    flag = 0\n",
        "    for item in fd[0]:\n",
        "      if item in key:\n",
        "        flag = 1\n",
        "        break\n",
        "    if flag == 0 or fd[0] == key:\n",
        "      correct_fds.append(fd)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #print(\"========= violated_fds =========\")\n",
        "  #print(violated_fds)\n",
        "  return violated_fds,correct_fds\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def is_in_3NF_internal(dependencies, headerList, candidateKey,faulty_dependency_indexes,key):\n",
        "    #print(\"===== 3NF dependencies =========\")\n",
        "    #print(dependencies)\n",
        "    transitive_dependencies = []\n",
        "    #print(\"====== global_map ==========\")\n",
        "    #print(global_map)\n",
        "    #print(candidateKey)\n",
        "    i=0\n",
        "    for fd in dependencies:\n",
        "      #print(\"====== fd ==========\")\n",
        "      #print(fd)\n",
        "      #====== fd ==========\n",
        "      #({'Course'}, {'CourseEnd', 'Professor', 'CourseStart'})\n",
        "      #====== fd ==========\n",
        "      #({'Professor'}, {'ProfessorEmail'})\n",
        "      lhs_attr = next(iter(fd[0]))\n",
        "      for ever_right in fd[1]:\n",
        "        if ever_right in key:\n",
        "          continue\n",
        "        #ever_right = professoremail\n",
        "        A = global_map[ever_right] #professor #course\n",
        "        if A is None:\n",
        "          continue\n",
        "        B = global_map[A] #course None\n",
        "        if B is None:\n",
        "          continue\n",
        "        if global_map[A] == B:\n",
        "          transitive_dependencies.append(({lhs_attr},{ever_right}))\n",
        "          faulty_dependency_indexes.setdefault(candidateKey, []).append(i)\n",
        "\n",
        "\n",
        "      i=i+1\n",
        "\n",
        "\n",
        "    return transitive_dependencies,faulty_dependency_indexes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Reading CSV\n",
        "    data = read_csv_file('Testing-BCNF-2.csv')\n",
        "    #print(\"============ data =============\")\n",
        "    ##print(data)\n",
        "\n",
        "    # Parsing Functional Dependencies\n",
        "    fds = parse_functional_dependencies()\n",
        "    #print(\"============ fds =============\")\n",
        "    #print(fds)\n",
        "\n",
        "    # Determine data types\n",
        "    #data_types = determine_data_types(data)\n",
        "\n",
        "    # User Input for Normalization Choice\n",
        "    #target_nf_choice = input(\"Choice of the highest normal form to reach (1: 1NF, 2: 2NF, 3: 3NF, B: BCNF, 4: 4NF, 5: 5NF): \")\n",
        "\n",
        "    # Determine current normal form\n",
        "    key = input(\"Key (can be composite): \").split(',')\n",
        "    #print(\"=============key=================\")\n",
        "    #print(key)\n",
        "\n",
        "    # User Input for Normalization Choice\n",
        "    target_nf_choice = input(\"Choice of the highest normal form to reach (1: 1NF, 2: 2NF, 3: 3NF, B: BCNF, 4: 4NF, 5: 5NF): \")\n",
        "    if target_nf_choice == '4':\n",
        "      mvds = find_multivalued_dependencies()\n",
        "      find_normal_form(data, fds, key,target_nf_choice,mvds)\n",
        "    else:\n",
        "      mvds = []\n",
        "      find_normal_form(data, fds, key,target_nf_choice,mvds)\n",
        "\n",
        "    if target_nf_choice == '5':\n",
        "      # Decompose the table into 5NF\n",
        "      decomposed_tables = decompose_to_5nf(data, fds)\n",
        "      print(\"5NF form of table : \")\n",
        "      decomposed_tables = [item for sublist in decomposed_tables for item in sublist]\n",
        "      print(pd.DataFrame(data)[decomposed_tables])\n",
        "\n",
        "# The result will be a list of decomposed tables, each in 5NF.\n",
        "\n",
        "\n",
        "    # Output the current normal form\n",
        "    #print(f\"The current normal form of the input table is: {current_nf}\")\n",
        "    #tables = [set(data[0].keys())]\n",
        "    #print(\"============ tables =============\")\n",
        "    #print(tables)\n",
        "\n",
        "def find_multivalued_dependencies():\n",
        "    mvds = []\n",
        "    while True:\n",
        "        mvd_input = input(\"Input Multi-Valued Dependencies (or 'exit'): \")\n",
        "        if mvd_input == \"exit\":\n",
        "            break\n",
        "        mvd = mvd_input.split('->>')\n",
        "        mvds.append((set(x.strip() for x in mvd[0].split(',')), set(x.strip() for x in mvd[1].split(','))))\n",
        "    return mvds\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eS454zubw5cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uZcXFhTVw51A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}